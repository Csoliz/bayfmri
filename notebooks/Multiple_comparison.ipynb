{
 "metadata": {
  "name": "Multiple comparison"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Multiple comparisons"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The problem occurs when you permform several statistical tests: by chance, some will be significant even if the data has been generated under the null hypothesis.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "FWER : Bonferroni"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Imaging you are doing $n$ t tests each one with a risk of type one error of $\\alpha$, and we denote by $t_\\alpha$ the t-value such that for a variable t drawn from a Student distribution, we have $P(t < t_\\alpha) = \\alpha$,   the probability of getting at least one significant test is these n tests is : "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$\n",
      "\\alpha_{F} = P(\\textrm{ one or more } t_i > t_\\alpha) = 1 - \\prod_{i} P(t_i < t_\\alpha) = 1 - (1 - \\alpha)^n \n",
      "$$ \n",
      "\n",
      "$$\n",
      "\\alpha = 1 -(1 - \\alpha_{F})^{\\frac{1}{n}} = 1 - \\exp\\left(\\frac{1}{n} \\log(1 - \\alpha_{F})\\right)\n",
      "$$\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# code demo:\n",
      "from scipy.stats import t as tdist\n",
      "df = 14\n",
      "n = 100\n",
      "alph = .05\n",
      "\n",
      "# Often easier to compute the static distribution of t with df : t14 = tdist(df)\n",
      "# rvs : draw one or several random variables\n",
      "ts = tdist(df).rvs(size=n)\n",
      "\n",
      "# The threshold as a t value: \n",
      "t_alph = tdist.isf(alph,df)\n",
      "\n",
      "print \"Nubmer of test significant at the %f level: %d \" %(alph, sum(ts >= t_alph))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Nubmer of test significant at the 0.050000 level: 4 \n"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To correct for the number of tests, we use :"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# if we have p values : \n",
      "p = tdist(df).sf(ts)\n",
      "p_corr = (1 - (1 - p)**n)\n",
      "print sum(p <= .05), sum(p_corr <= .05)\n",
      "\n",
      "def p_corrected(ps):\n",
      "    return( 1 - (1 - p)**n )\n",
      "\n",
      "# alternatively : compare p with \n",
      "print 1.0 - exp(1./n * log(1 - .05))\n",
      "\n",
      "def alph_corr(alph):\n",
      "    return 1.0 - exp(1./n * log(1 - alph))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "99 1\n",
        "5.1291978909e-05\n"
       ]
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "On average we would have to run about 20 times the previous code to get a (spurious) significant effect.\n",
      "\n",
      "Note that when the tests are NOT INDEPENDENT, then this procedure is conservative (on average you will get less false positive than the specified risk or error), and therefore not the most sensitive. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "FWER : RFT"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "False Detection Rate "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The false detection rate (introduced by Benjamini and Hochberg (1995), see [http://en.wikipedia.org/wiki/False_discovery_rate] for an history of this measure."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's denote : \n",
      "\n",
      "* $S$ the number of true positive\n",
      "* $V$ the number of false positives (Type I error)\n",
      "* $R$ is the number of rejected null hypotheses (also called \"discoveries\")\n",
      "* $Q$ the proportion of false discoveries amongst the discoveries = $\\frac{V}{S+V}$\n",
      "\n",
      "We want to control such that $Q = \\frac{V}{S+V} = \\frac{V}{R}$ is small. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The procedure : \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = 14\n",
      "t14 = tdist(df)\n",
      "n = 1000\n",
      "\n",
      "ts = t14.rvs(size=n)\n",
      "ts = np.sort(ts) # careful of ts = ts.sort() .... you get None !!!\n",
      "ts[:-int(n*.1):-1] += 1\n",
      "ps = t14.sf(ts)\n",
      "\n",
      "#uncorrected\n",
      "print  'Uncorrected: ',sum(ps < .05)\n",
      "\n",
      "#Bonferroni\n",
      "print 'Bonferroni: ',sum(ps < alph_corr(.05))\n",
      "\n",
      "#FDR\n",
      "#q = ( (range(n)+1)/n) * 0.05 ) \n",
      "# adapted from nipy/algo/stat/empirical_pvalue\n",
      "def fdrcurve(self):\n",
      "        \"\"\"\n",
      "        Returns the FDR associated with any point of self.x\n",
      "        \"\"\"\n",
      "        import scipy.stats as st\n",
      "        if self.learned == 0:\n",
      "            self.learn()\n",
      "        efp = (self.p0 * st.norm.sf(self.x, self.mu, self.sigma)\n",
      "               * self.n / np.arange(self.n, 0, - 1))\n",
      "        efp = np.minimum(efp, 1)\n",
      "        ix = np.argsort(self.x)\n",
      "        for i in range(np.size(efp) - 1, 0, - 1):\n",
      "            efp[ix[i - 1]] = np.maximum(efp[ix[i]], efp[ix[i - 1]])\n",
      "        self.sorted_x = self.x[ix]\n",
      "        self.sorted_fdr = efp[ix]\n",
      "        return efp\n",
      "\n",
      "\n",
      "#print np.argmax( \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Uncorrected:  99\n",
        "Bonferroni:  0\n"
       ]
      }
     ],
     "prompt_number": 60
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A very rough explanation of the procedure. \n",
      "\n",
      "If we do m tests at 0.05, we expect to set up the corrected p value at about 0.05/m.\n",
      "so if the test with the smallest p value is less than this, we can assume that we have one true positive. \n",
      "Therefore, we should correct for only (m-1) tests. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}